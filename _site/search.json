[
  {
    "objectID": "proposal/proposal.html",
    "href": "proposal/proposal.html",
    "title": "Proposal",
    "section": "",
    "text": "In Singapore, while there has been significant research on climate change, the relationship between changing weather patterns and air quality has not been thoroughly explored. While The National Environment Agency in Singapore has made historical weather and air quality data accessible, there remains a shortage of user-friendly tools to effectively visualize and analyze this data for a deeper understanding of its implications. Exploring these datasets within a visual analytics environment can provide valuable insights for various users, from policymakers to the general public."
  },
  {
    "objectID": "proposal/proposal.html#motivation",
    "href": "proposal/proposal.html#motivation",
    "title": "Proposal",
    "section": "",
    "text": "In Singapore, while there has been significant research on climate change, the relationship between changing weather patterns and air quality has not been thoroughly explored. While The National Environment Agency in Singapore has made historical weather and air quality data accessible, there remains a shortage of user-friendly tools to effectively visualize and analyze this data for a deeper understanding of its implications. Exploring these datasets within a visual analytics environment can provide valuable insights for various users, from policymakers to the general public."
  },
  {
    "objectID": "proposal/proposal.html#the-problems-addressed",
    "href": "proposal/proposal.html#the-problems-addressed",
    "title": "Proposal",
    "section": "The problems addressed",
    "text": "The problems addressed\nThere are some visualizations made available by the government for climate trends and air quality through websites like Weather.gov.sg and the Haze Information Portal. However, these tools have some limitations:\n\nThe current tools predominantly feature basic, static visualizations fixed to specific timeframes, such as choropleth maps or maps with callouts. They lack interactivity that can display the spatial and temporal nature of the data across different areas in Singapore.\nThere are no available tools with advanced analytical functions, such as anomaly detection, time series analysis, and future projection. Users are limited in observing trends in climate or air quality.\nMoreover, there is no integrated tool to explore, visualize and analyse the relationship between air pollutants and weather parameters (temperature, wind, and rainfall).\n\nThis project aims to overcome the identified problems by developing an intuitive and visual analytics tool that allows one to explore historical weather and air quality in Singapore."
  },
  {
    "objectID": "proposal/proposal.html#relevant-related-work",
    "href": "proposal/proposal.html#relevant-related-work",
    "title": "Proposal",
    "section": "Relevant related work",
    "text": "Relevant related work\nThis section will briefly discuss on some of relevant works that have already been done regarding weather and air quality.\n\nIn Haze Information Portal website managed by National Environment Agency (NEA), air quality in Singapore is displayed through Pollutant Standards Index (PSI) and PM 2.5 indicators. The visualization presents current air quality condition in an hourly and a 24-hour overview. Also, they have categorized air quality into different groups for users to easily understand the current conditions. However, while the platform allows users to display air quality in specified areas immediately, it limits them to conduct comparative analysis across different regions or scope into historical data trends. Furthermore, the platform lacks forecasting tools which could enhance users more insightful information.\n\n\n\nMeteorological Service Singapore website has a section to view historical daily weather record in Singapore. Users can select various regions and retrieve climate data from different months and years. The interactive map provides a geographical plot of the data for users to select the areas. However, it lacks time series visualization that would allow users to analyse weather trends over multiple years. Moreover, the platform also does not support comparative analysis between different area in which it would be useful for understanding climate variation across different areas within Singapore.\n\n\n\nWorld Weather map website provides an interactive global map that shows an information of various weather parameters, including temperature, precipitation, humidity, pressure, cloud, visibility, and wind. It also offers historical weather data for the past five days. This is a valuable resource for users to understand recent weather trends. However, the platform does not offer data specific to different areas in Singapore. Additionally, there is no forecast data and comparative analysis tools. Thus, it restricts users to anticipate future weather conditions or compare weather trends across different locations in Singapore."
  },
  {
    "objectID": "proposal/proposal.html#our-solution",
    "href": "proposal/proposal.html#our-solution",
    "title": "Proposal",
    "section": "Our solution",
    "text": "Our solution\nUsing various R packages, we will analyse and visualize weather and air quality across different areas in Singapore and build an interactive R Shiny application. Users will be able to:\n\nInteractively explore customizable and dynamic visualizations that display weather and air quality data over time and across different regions in Singapore.\nUse selected analytical functions to highlight key patterns, identify anomalies, and make projections about future climate and air quality.\nExplore and visualize the interrelationship between various air pollutants and weather parameters like temperature, wind speed, and rainfall."
  },
  {
    "objectID": "proposal/proposal.html#the-data",
    "href": "proposal/proposal.html#the-data",
    "title": "Proposal",
    "section": "The data",
    "text": "The data\nThe following datasets will be used:\n\n\n\n\n\n\n\n\n\nDataset\nFrequency\nPeriod\nSource\n\n\n\n\nClimate Historical Daily Records for various stations in Singapore\n(rainfall, temperature and wind speed)\nDaily\nPeriod varies across stations but approx. 2014 – 2023\nhttp://www.weather.gov.sg/climate-historical-daily/\n\n\nAir quality for 5 locations in Singapore\n(PM2.5, PM10, O3, NO2, SO2, CO, PSI)\nDaily\n2014 - 2023\nhttp://www.weather.gov.sg/climate-historical-daily/\n\n\nAir Temperature And Sunshine, Relative Humidity And Rainfall\n(Changi climate station)\nMonthly\nJan 1975 – Dec 2023\nM890081_monthly_temp&rain_197501-202312.csv\nhttps://tablebuilder.singstat.gov.sg/table/TS/M890081"
  },
  {
    "objectID": "proposal/proposal.html#approach-and-prototypes",
    "href": "proposal/proposal.html#approach-and-prototypes",
    "title": "Proposal",
    "section": "Approach and prototypes",
    "text": "Approach and prototypes\n\n1. Bird’s Eye View of Weather and Air Quality\nThis module will show dynamic, interactive map with option to include both weather and air quality data across 5 key areas in Singapore. Users will be able to select different time frames and regions for detailed examination.\nPrototype Features:\n\nTab 1: Map\n\nInteractive mapping tool with zoom and pan functions.\nLayer selection for various weather and air quality parameters.\nTime slider to view data across different historical periods.\nRegion-specific data display upon selection.\n\n\n\n\nTab 2: Plots\n\nOptions to select distribution plots for key weather parameters and pollutants by area, including density plots, histograms, and boxplots.\nComparative visualizations with side-by-side evaluations of multiple different areas or variables.\nCustomizable granularity to observe either monthly or yearly view of the data.\n\n\n\nFollowing R packages will be used in this section:\n\ntidyverse (CRAN - Package tidyverse (r-project.org)): An ecosystem of packages designed for data science that makes it easy to tidy, transform, and visualize data.\nggplot2 (CRAN - Package ggplot2 (r-project.org)): A powerful package for creating static, aesthetic, and informative graphics.\nplotly (CRAN - Package plotly (r-project.org)): Provides a high-level interface to Plotly, enabling the creation of interactive and dynamic visualizations.\nDT (CRAN - Package DT (r-project.org)): An interface to the JavaScript library DataTables, used to render interactive and rich data tables.\nsf (CRAN - Package sf (r-project.org)): An R package for handling and analyzing spatial data with simple features, which can interface with leaflet for mapping.\n\n\n\n2. Time-series Toolkit\nThis module enables users to forecast future climate conditions using historical data from 1975 to 2023. It offers interactive tools for model selection, prediction adjustments, and visualizes forecasts alongside historical data.\nPrototype Features:\n\nModel selection from a list of forecasting models (e.g., Linear Regression, ARIMA, Prophet) via checkbox\nFlexible selection for variables, period, forecast horizon and prediction parameters.\nDynamic Prediction Charts to Visualize past trends and future projections.\n\nFollowing R packages will be used in this section:\n\nLubridate (CRAN - Package lubridate (r-project.org)) : fast and user friendly parsing of date-time data, extraction and updating of components of a date-time\nTimetk (CRAN - Package timetk (r-project.org)) : Easy visualization, wrangling, and feature engineering of time series data for forecasting and machine learning prediction.\nModeltime (CRAN - Package modeltime (r-project.org)) : Time series forecasting framework with models including ARIMA, Exponential Smoothing, and additional time series models from the ‘forecast’ and ‘prophet’ packages\n\n\n\n\n3. Climate-Pollutant Relationship Explorer\nThis module would enable the user to select different types of plots and variables to be included in plots centered on exploring relationships. Below, we list some possible plots for such analysis:\n\nBubble Plot\nThis bubble plot, displaying the relationship between daily rainfall, mean temperature, and PM2.5 levels (represented by bubble size), provides a multidimensional view of the data. The plot allows a user to:\n\nassess if there are any trends between rainfall and temperature\nobserve how air quality varies with different weather conditions. For example, days with higher rainfall has smaller bubbles (lower PM2.5 levels) in the plot below, suggesting that rain could help reduce air pollution.\n\n\n\n\nTreemap\nThis treemap can convey both the PM2.5 pollution levels (through the size of the rectangles) and the mean temperature (through the color of the rectangles) across different areas and stations.\n\n\n\n\n\n\nWarning\n\n\n\nNote that only a small number of selected stations were grouped into different areas for this prototype"
  },
  {
    "objectID": "code/Data_preparation_Airquality.html",
    "href": "code/Data_preparation_Airquality.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Objective of this exercise is to perform data preparation for our air quality datasets."
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#overview",
    "href": "code/Data_preparation_Airquality.html#overview",
    "title": "Data Preparation",
    "section": "",
    "text": "Objective of this exercise is to perform data preparation for our air quality datasets."
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#getting-started",
    "href": "code/Data_preparation_Airquality.html#getting-started",
    "title": "Data Preparation",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 Load libraries\nFirst, we load packages required:\n\ntidyverse: to support tasks such as reading in packages or data manipulation\nnaniar: for using miss_vis() function to check data for missing values\nimputeTS: for using na_ma() function to impute missing values\nDT: for using datatable() to view the dataset interactively\nlubridate: for handling date-time formats\n\n\npacman::p_load(tidyverse, naniar,imputeTS, DT, lubridate)\n\n\n\n2.2 Import datasets\nNext we import the datasets.\n\npollutants &lt;- read_csv(\"../data/pollutants_singapore.csv\", locale = locale(encoding = \"ISO-8859-1\"))\n\npm25_hourly &lt;- read_csv(\"../data/Historical1hrPM2.5.csv\", locale = locale(encoding = \"ISO-8859-1\"))\n\npsi_24h &lt;- read_csv(\"../data/Historical24hrPSI.csv\", locale = locale(encoding = \"ISO-8859-1\"))\n\nFor air quality, we have the following datasets:\n\npollutants: Daily concentration of 6 air pollutants and PSI level for 5 areas in Singapore, for the period of approximately 2014 to 2023.\npm25_hourly: Historical regional 1-hr PM2.5 value measured in µg/m3, for period April 2014 to Dec 2022. Retrieved from Data.gov.sg: Historical 1-hr PM2.5.\npsi_24h: Historical regional 24-hr PSI, for period April 2014 to Dec 2022. Retrieved from Data.gov.sg: Historical 24-hr PSI."
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#data-preparation-pollutants",
    "href": "code/Data_preparation_Airquality.html#data-preparation-pollutants",
    "title": "Data Preparation",
    "section": "3 Data Preparation: pollutants",
    "text": "3 Data Preparation: pollutants\n\n3.1 Overview\nBelow is the summary of data preparation steps:\n\nCheck data structure\nConvert date from chr to Date type\nFilter for desired period, 2021 to 2023\nCheck for duplicated rows\nCheck for missing values\nDrop columns with large number of missing values\nImpute missing values for remaining columns\nSave data to rds\n\n\n\n3.2 Check structure with glimpse()\n\nglimpse(pollutants)\n\nRows: 18,390\nColumns: 9\n$ Area &lt;chr&gt; \"Central\", \"Central\", \"Central\", \"Central\", \"Central\", \"Central\",…\n$ date &lt;chr&gt; \"2024/2/1\", \"2024/2/2\", \"2024/2/3\", \"2024/1/1\", \"2024/1/2\", \"2024…\n$ pm25 &lt;dbl&gt; 46, 48, 46, 35, 25, 37, 26, 26, 31, 24, 34, 26, 25, 46, 24, 50, 4…\n$ pm10 &lt;dbl&gt; 25, 29, NA, 16, 18, 16, 13, 21, 17, 14, 17, 14, 21, 25, 21, 36, 2…\n$ o3   &lt;dbl&gt; 10, 10, NA, 8, 9, 8, 12, 16, 8, 14, 12, 9, 9, 37, 12, 13, 11, 8, …\n$ no2  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ so2  &lt;dbl&gt; 2, 2, NA, 1, 1, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 3, 1, 1…\n$ co   &lt;dbl&gt; 5, 5, NA, 3, 2, 4, 3, 5, 5, 6, 5, 4, 7, 7, 8, 7, 6, 3, 3, 3, 4, 4…\n$ psi  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThere are 18, 390 rows, and 9 columns in the dataset. Here are more details on the 9 columns:\n\nArea : The ambient air quality in Singapore is continuously monitored through a network of air monitoring sensors across the island. The data from the air monitoring sensors are reported for five regions in Singapore – North, South, East, West, Central.\ndate : The date for each row is recorded in YYYY/MM/DD format, of type chr.\n\nWe will need to convert this to date/time class e.g. Date, POSIXct, or POSIXlt.\nWe will also select the time period of 2021-2023, to align the time period of this dataset with the weather dataset.\n\npm25, pm10, o3, no2, so2, co : The air monitoring sensors measure concentration levels of six air pollutants: fine particulate matter (PM2.5), particulate matter (PM10), ozone (O3), nitrogen dioxide (NO2), sulphur dioxide (SO2) and carbon monoxide (CO). The values are reported in µg/m3 for each 24h period / day.\npsi : PSI is the air quality index used in Singapore. It is reported as a number on a scale of 0 to 500.\n\n\n\n3.3 Convert date column to date type\nThe code below converts the date column to type ‘Date’ and renames it to Date.\n# Check that date column is now date type\nclass(pollutants$Date)\n\n# Convert date column from character to date type\npollutants &lt;- pollutants %&gt;%\n  mutate(date = trimws(date), \n         date = as.Date(date, format=\"%Y/%m/%d\"))\n\n# Rename date column top 'Date'\npollutants &lt;- pollutants %&gt;% \n       rename(\"Date\" = \"date\")\n\n# Check that date column is now date type\nclass(pollutants$Date)\n\n[1] \"Date\"\n\n\n\n\n3.4 Filter desired period, 2021 to 2023\nFor this project, we focus on the year 2021 to 2023.\n\npollutants &lt;- pollutants %&gt;%\n  filter(between(Date, as.Date('2021-01-01'), as.Date('2023-12-31')))\n\n# Check time period of data\ntime_period_start &lt;- min(pollutants$Date)\ntime_period_end &lt;- max(pollutants$Date)\n\n# Print the time period\ncat(\"The time period of the filtered dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\nThe time period of the filtered dataset is from 2021-01-01 to 2023-12-31 \n\n\n\n\n3.5 Check for duplicated rows\nWe use the following code to check for duplicated data based on combination of Area and Date. If there is any duplicated data, it will be shown.\n\n# Identify duplicates\nduplicates &lt;- pollutants[duplicated(pollutants[c(\"Area\", \"Date\")]) | duplicated(pollutants[c(\"Area\", \"Date\")], fromLast = TRUE), ]\n\n# Check if 'duplicates' dataframe is empty\nif (nrow(duplicates) == 0) {\n  print(\"The combination of Area and Date is unique.\")\n} else {\n  print(\"There are duplicates. Showing duplicated rows:\")\n  # Print out the duplicated rows\n  print(duplicates)\n}\n\n[1] \"The combination of Area and Date is unique.\"\n\n\nThere are no duplicated rows in our dataset.\n\n\n3.6 Check for missing values\nWe will first visually assess if there are any missing values using vis_miss() from the naniar package.\n\nvis_miss(pollutants)\n\n\n\n\n\n\n\n\nWe see that there are almost no values for two columns, no2 and psi. We will drop these columns in the next step. There are also missing values for pm25, pm10, o3, so2, co columns. We will select a suitable imputation method to handle these missing values as well.\n\n\n3.7 Drop columns with large number of missing values\nIn the code below, we drop the columns no2 and psi.\n\npollutants &lt;- pollutants %&gt;%\n  select(-c(no2, psi))\n\n\n\n3.8 Impute missing values\nTo handle the missing values for pm25, pm10, o3, so2, co columns, we will impute missing values using simple moving average from imputeTS package.\n\n# List of pollutants\npollutant_variables &lt;- c(\"pm25\", \"pm10\", \"o3\", \"so2\", \"co\")\n\n# Loop through each pollutant variable to impute missing values\nfor(variable in pollutant_variables) {\n  # Apply the imputation by grouping by Area and arranging by Date\n  pollutants &lt;- pollutants %&gt;%\n    group_by(Area) %&gt;%\n    arrange(Area, Date) %&gt;%\n    mutate(!!variable := na_ma(!!sym(variable), k = 3, weighting = \"simple\")) %&gt;%\n    ungroup()\n}\n\n# Visual check for missing values\nvis_miss(pollutants)\n\n\n\n\n\n\n\n\n\n\n3.9 View dataset and data structure\n\n# Dataset structure\nstr(pollutants)\n\ntibble [5,475 × 7] (S3: tbl_df/tbl/data.frame)\n $ Area: chr [1:5475] \"Central\" \"Central\" \"Central\" \"Central\" ...\n $ Date: Date[1:5475], format: \"2021-01-01\" \"2021-01-02\" ...\n $ pm25: num [1:5475] 22 19 26 28 27 28 26 34 29 19 ...\n $ pm10: num [1:5475] 17 16 18 19 21 20 23 28 20 19 ...\n $ o3  : num [1:5475] 9 3 6 10 7 4 5 9 12 16 ...\n $ so2 : num [1:5475] 2 2 2 2 2 2 2 2 2 2 ...\n $ co  : num [1:5475] 4 3.25 3 3 3 4 3 3 4 3 ...\n\n\n\ndatatable(pollutants, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 31,scrollX=T),\n          caption = 'polluants data set after imputation, 2021 - 2023')\n\n\n\n\n\n\n\n3.10 Save data to rds\nFollowing code is used to export the imputed data table to csv.\n\nwrite_rds(pollutants, \"../data/pollutants_imputed.rds\")\n\nTo read data in the future, use the following code chunk:\n\npollutants_data &lt;- read_rds(\"../data/pollutants_imputed.rds\")"
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#data-preparation-pm25_hourly",
    "href": "code/Data_preparation_Airquality.html#data-preparation-pm25_hourly",
    "title": "Data Preparation",
    "section": "4 Data Preparation: pm25_hourly",
    "text": "4 Data Preparation: pm25_hourly\n\n4.1 Overview\nBelow is the summary of data preparation steps:\n\nCheck data structure\nConvert column time of datetime column\nFilter for desired period, 2020 to 2022\nRename columns to be TitleCase\nCheck for duplicated rows\nCheck for missing values\nSave data to rds\n\n\n\n4.2 Check structure with glimpse()\n\nglimpse(pm25_hourly)\n\nRows: 76,728\nColumns: 6\n$ `1hr_pm2.5` &lt;chr&gt; \"1/4/2014 1:00\", \"1/4/2014 2:00\", \"1/4/2014 3:00\", \"1/4/20…\n$ north       &lt;dbl&gt; 7, 10, 15, 21, 28, 28, 37, 19, 19, 21, 20, 18, 13, 24, 22,…\n$ south       &lt;dbl&gt; 6, 10, 20, 14, 12, 31, 24, 16, 17, 16, 15, 22, 12, 17, 19,…\n$ east        &lt;dbl&gt; 7, 10, 15, 27, 28, 21, 27, 25, 18, 20, 16, 11, 12, 17, 14,…\n$ west        &lt;dbl&gt; 23, 36, 31, 36, 35, 33, 35, 19, 25, 25, 25, 22, 23, 28, 19…\n$ central     &lt;dbl&gt; 8, 10, 15, 12, 22, 24, 28, 37, 22, 14, 20, 22, 11, 23, 18,…\n\n\n\n1hr_pm2.5 : The datetime for each row is recorded in YYYY/MM/DD format, of type chr.\n\nWe will need to convert this to date/time class\nWe will rename it to ‘DateTime’.\nWe will also select the time period of 2021-2022 for this project.\n\nnorth, south, east, west, central : Each column contains data from the air monitoring sensors reported for a region in Singapore.\n\nWe will rename the columns to capitalize the first letter of each region name, e.g. ‘East’\n\n\n\n\n4.3 Convert 1hr_pm2.5 column to POSIXct type\nWe will use dmy_hm() function from lubridate package:\n\npm25_hourly &lt;- pm25_hourly %&gt;%\n  rename(DateTime = `1hr_pm2.5`) %&gt;% # Rename '1hr_pm2.5' to 'DateTime'\n  mutate(DateTime = dmy_hm(DateTime)) # Convert 'DateTime' from character to POSIXct, assuming the time is in format \"YYYY/MM/DD HH:MM\"\n\n# Check that date column is now correct type\nclass(pm25_hourly$DateTime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n\n\n4.4 Filter desired period, 2020 to 2022\n\npm25_hourly &lt;- pm25_hourly %&gt;%\n  filter(year(DateTime) &gt;= 2020, year(DateTime) &lt;= 2022) # Filter for the time period of 2021-2022\n\n# Check time period of data\ntime_period_start &lt;- min(pm25_hourly$DateTime)\ntime_period_end &lt;- max(pm25_hourly$DateTime)\n\n# Print the time period\ncat(\"The time period of the filtered dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\nThe time period of the filtered dataset is from 2020-01-01 to 2022-12-31 \n\n\n\n\n4.5 Rename columns\nWe will rename the columns to capitalize the first letter of each region name, e.g. ‘East’\n\npm25_hourly &lt;- pm25_hourly %&gt;%\n  rename_with(~tools::toTitleCase(.), .cols = c(\"north\", \"south\", \"east\", \"west\", \"central\"))\n\n# Check the new column names\ncolnames(pm25_hourly)\n\n[1] \"DateTime\" \"North\"    \"South\"    \"East\"     \"West\"     \"Central\" \n\n\n\n\n4.6 Check for duplicated rows\nWe use the following code to check for duplicated data. If there is any duplicated data, it will be shown.\n\n# Check for duplicate rows\nduplicates &lt;- pm25_hourly %&gt;%\n  group_by(across(everything())) %&gt;%   # Group by all columns\n  filter(n() &gt; 1) %&gt;%                  # Filter groups with more than one row\n  ungroup()                            # Remove the grouping\n\n# Count the number of duplicate rows\nnum_duplicates &lt;- nrow(duplicates)\n\n# Print the number of duplicates\nprint(paste(\"Number of duplicate rows:\", num_duplicates))\n\n[1] \"Number of duplicate rows: 0\"\n\n\nThere are no duplicated rows in our dataset.\n\n\n4.7 Check for missing values\nWe will first visually assess if there are any missing values using vis_miss() from the naniar package.\n\nvis_miss(pm25_hourly)\n\n\n\n\n\n\n\n\nThere is no missing data.\n\n\n4.8 View dataset structure and data\n\nstr(pm25_hourly)\n\ntibble [26,304 × 6] (S3: tbl_df/tbl/data.frame)\n $ DateTime: POSIXct[1:26304], format: \"2020-01-01 00:00:00\" \"2020-01-01 01:00:00\" ...\n $ North   : num [1:26304] 5 9 11 9 11 7 7 9 9 8 ...\n $ South   : num [1:26304] 7 4 13 5 8 7 7 8 8 9 ...\n $ East    : num [1:26304] 9 7 8 11 9 7 11 7 18 15 ...\n $ West    : num [1:26304] 5 13 4 4 7 14 3 6 9 3 ...\n $ Central : num [1:26304] 9 7 7 5 11 11 6 10 13 10 ...\n\n\n\ndatatable(pm25_hourly, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 24,scrollX=T),\n          caption = 'pm25_hourly: Historical regional 1-hr PM2.5 value (µg/m3) for 2020 - 2022')\n\n\n\n\n\n\n\n4.9 Save data to rds\nFollowing code is used to export the imputed data table to csv.\n\nwrite_rds(pm25_hourly, \"../data/pm25_hourly.rds\")\n\nTo read data in the future, use the following code chunk:\n\npm25_hourly_data &lt;- read_rds(\"../data/pm25_hourly.rds\")"
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#data-preparation-psi_24h",
    "href": "code/Data_preparation_Airquality.html#data-preparation-psi_24h",
    "title": "Data Preparation",
    "section": "5 Data Preparation: psi_24h",
    "text": "5 Data Preparation: psi_24h\n\n5.1 Overview\nBelow is the summary of data preparation steps:\n\nCheck data structure\nConvert column time of datetime column\nFilter for desired period, 2020 to 2022\nRename columns to be TitleCase\nCheck for duplicated rows\nCheck for missing values\nSave data to rds\n\n\n\n5.2 Check structure with glimpse()\n\nglimpse(psi_24h)\n\nRows: 76,728\nColumns: 6\n$ `24-hr_psi` &lt;chr&gt; \"1/4/2014 1:00\", \"1/4/2014 2:00\", \"1/4/2014 3:00\", \"1/4/20…\n$ north       &lt;dbl&gt; 55, 55, 55, 56, 57, 58, 59, 60, 59, 60, 60, 60, 59, 59, 59…\n$ south       &lt;dbl&gt; 54, 54, 55, 55, 55, 56, 57, 57, 57, 58, 57, 57, 56, 56, 56…\n$ east        &lt;dbl&gt; 54, 54, 54, 55, 56, 56, 57, 58, 58, 58, 59, 58, 58, 58, 57…\n$ west        &lt;dbl&gt; 58, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 67, 67, 67, 67…\n$ central     &lt;dbl&gt; 54, 54, 55, 55, 56, 56, 57, 58, 59, 59, 59, 59, 59, 59, 59…\n\n\n\n24-hr_psi : The datetime for each row is recorded in YYYY/MM/DD format, of type chr.\n\nWe will need to convert this to date/time class\nWe will rename it to ‘DateTime’.\nWe will also select the time period of 2021-2022 for this project.\n\nnorth, south, east, west, central : Each column contains data from the air monitoring sensors reported for a region in Singapore.\n\nWe will rename the columns to capitalize the first letter of each region name, e.g. ‘East’\n\n\n\n\n5.3 Convert psi_24h column to POSIXct type\nWe will use dmy_hm() function from lubridate package:\n\npsi_24h &lt;- psi_24h %&gt;%\n  rename(DateTime = `24-hr_psi`) %&gt;% # Rename '24-hr_psi' to 'DateTime'\n  mutate(DateTime = dmy_hm(DateTime)) # Convert 'DateTime' from character to POSIXct, assuming the time is in format \"YYYY/MM/DD HH:MM\"\n\n# Check that date column is now correct type\nclass(pm25_hourly$DateTime)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n\n\n5.4 Filter desired period, 2020 to 2022\n\npsi_24h &lt;- psi_24h %&gt;%\n  filter(year(DateTime) &gt;= 2020, year(DateTime) &lt;= 2022) # Filter for the time period of 2021-2022\n\n# Check time period of data\ntime_period_start &lt;- min(psi_24h$DateTime)\ntime_period_end &lt;- max(psi_24h$DateTime)\n\n# Print the time period\ncat(\"The time period of the filtered dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\nThe time period of the filtered dataset is from 2020-01-01 to 2022-12-31 \n\n\n\n\n5.5 Rename columns\nWe will rename the columns to capitalize the first letter of each region name, e.g. ‘East’\n\npsi_24h &lt;- psi_24h %&gt;%\n  rename_with(~tools::toTitleCase(.), .cols = c(\"north\", \"south\", \"east\", \"west\", \"central\"))\n\n# Check the new column names\ncolnames(psi_24h)\n\n[1] \"DateTime\" \"North\"    \"South\"    \"East\"     \"West\"     \"Central\" \n\n\n\n\n5.6 Check for duplicated rows\nWe use the following code to check for duplicated data. If there is any duplicated data, it will be shown.\n\n# Check for duplicate rows\nduplicates &lt;- psi_24h %&gt;%\n  group_by(across(everything())) %&gt;%   # Group by all columns\n  filter(n() &gt; 1) %&gt;%                  # Filter groups with more than one row\n  ungroup()                            # Remove the grouping\n\n# Count the number of duplicate rows\nnum_duplicates &lt;- nrow(duplicates)\n\n# Print the number of duplicates\nprint(paste(\"Number of duplicate rows:\", num_duplicates))\n\n[1] \"Number of duplicate rows: 0\"\n\n\nThere are no duplicated rows in this dataset.\n\n\n5.7 Check for missing values\nWe will first visually assess if there are any missing values using vis_miss() from the naniar package.\n\nvis_miss(psi_24h)\n\n\n\n\n\n\n\n\nThere is no missing data.\n\n\n5.8 View dataset structure and data\n\nstr(psi_24h)\n\ntibble [26,304 × 6] (S3: tbl_df/tbl/data.frame)\n $ DateTime: POSIXct[1:26304], format: \"2020-01-01 00:00:00\" \"2020-01-01 01:00:00\" ...\n $ North   : num [1:26304] 33 33 33 33 34 34 34 34 34 34 ...\n $ South   : num [1:26304] 35 34 35 35 35 34 34 35 34 34 ...\n $ East    : num [1:26304] 35 35 34 35 35 34 34 34 36 37 ...\n $ West    : num [1:26304] 37 38 37 36 35 36 34 34 34 33 ...\n $ Central : num [1:26304] 36 35 35 34 34 35 35 35 36 37 ...\n\n\n\ndatatable(psi_24h, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 24,scrollX=T),\n          caption = 'psi_24h: Historical regional 24-hr PSI, for 2020 - 2022')\n\n\n\n\n\n\n\n5.9 Save data to rds\nFollowing code is used to export the imputed data table to csv.\n\nwrite_rds(psi_24h, \"../data/psi_24h.rds\")\n\nTo read data in the future, use the following code chunk:\n\npsi_24h_data &lt;- read_rds(\"../data/psi_24h.rds\")"
  },
  {
    "objectID": "code/Data_preparation_Airquality.html#appendix",
    "href": "code/Data_preparation_Airquality.html#appendix",
    "title": "Data Preparation",
    "section": "6 Appendix",
    "text": "6 Appendix\n\n6.1 Understanding PM2.5\nAccording to Singapore’s National Environment Agency (NEA), 1-hour PM2.5 values can be used as indication of air quality. However, the PM2.5 values in our pollutants dataset are daily values, not 1-hour values. Still, we will note that the PM2.5 values can be grouped by their concentration level:\n\n\n\n6.2 Understanding PSI\nPSI can be grouped by index values and descriptors, explaining the effects of the levels, according to Singapore’s National Environment Agency (NEA).\n\n\n\n\n\n\n\n\nPSI\nDescriptor\nGeneral Health Effects\n\n\n\n\n0–50\nGood\nNone\n\n\n51–100\nModerate\nFew or none for the general population\n\n\n101–200\nUnhealthy\nEveryone may begin to experience health effects; members of sensitive groups may experience more serious health effects. To stay indoors.\n\n\n201–300\nVery unhealthy\nHealth warnings of emergency conditions. The entire population is more likely to be affected.\n\n\n301+\nHazardous\nHealth alert: everyone may experience more serious health effects\n\n\n\nThe PSI is computed based on the 24-hour average of concentration levels of 6 pollutants. A sub-index value is computed for each pollutant based on the pollutant’s ambient air concentration. The highest sub-index value is then taken as the PSI value. In other words, the PSI is determined by the pollutant with the most significant concentration. Technical details on how the PSI is calculated can be found here: computation of PSI.\n\n\n\n\n\n\nNote\n\n\n\nFor our pollutants dataset, due to the unavailability of PSI data for the time period, we will not use this variable."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the team",
    "section": "",
    "text": "Coming soon~"
  },
  {
    "objectID": "code/Data_preparation_Weather.html",
    "href": "code/Data_preparation_Weather.html",
    "title": "Data Preparation",
    "section": "",
    "text": "In this document we clean and prepare our weather data.\n\nflowchart TD\n    A(Import Data) --&gt; B(Data preparation)\n    B --&gt; C(Save data)\n\n\n\n\nflowchart TD\n    A(Import Data) --&gt; B(Data preparation)\n    B --&gt; C(Save data)"
  },
  {
    "objectID": "code/Data_preparation_Weather.html#overview",
    "href": "code/Data_preparation_Weather.html#overview",
    "title": "Data Preparation",
    "section": "",
    "text": "In this document we clean and prepare our weather data.\n\nflowchart TD\n    A(Import Data) --&gt; B(Data preparation)\n    B --&gt; C(Save data)\n\n\n\n\nflowchart TD\n    A(Import Data) --&gt; B(Data preparation)\n    B --&gt; C(Save data)"
  },
  {
    "objectID": "code/Data_preparation_Weather.html#getting-started",
    "href": "code/Data_preparation_Weather.html#getting-started",
    "title": "Data Preparation",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 Load R packages\nFirst, we load packages required:\n\npacman::p_load(naniar,\n               tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, ggpattern,\n               hrbrthemes, plotly,\n               sf, tmap,\n               lubridate,\n               DT, imputeTS, \n               kableExtra)"
  },
  {
    "objectID": "code/Data_preparation_Weather.html#import-weather-data",
    "href": "code/Data_preparation_Weather.html#import-weather-data",
    "title": "Data Preparation",
    "section": "3 Import weather data",
    "text": "3 Import weather data\n\nweather &lt;- read_csv(\"../data/climate_historical_daily_records.csv\")"
  },
  {
    "objectID": "code/Data_preparation_Weather.html#data-preparation",
    "href": "code/Data_preparation_Weather.html#data-preparation",
    "title": "Data Preparation",
    "section": "4 Data preparation",
    "text": "4 Data preparation\nFor this project, we retrieved data from the NEA website for 63 weather stations distributed across Singapore from the years 2014 to 2023. After examining the station records file, we found that 41 stations had some missing information for a few variables. Additionally, we identified stations that had no recorded data for entire months and others with excessive missing values, indicating significant gaps in data collection and recording. The detailed data cleaning process are documented down in the next few sections.\n\nflowchart LR\n    A(Data preparation) --&gt; B(Filter for period\\n2021 to 2023)\n    A --&gt; C(Remove unused variables)\n    A --&gt; D(Check and remove duplicates)\n    A --&gt; E(Station-specific cleaning)\n    E --&gt; F(Remove stations with\\nincomplete data)\n    E --&gt; G(Remove stations with\\nno data for an entire month)\n    E --&gt; H(Remove stations with \\nexcessive missing values)\n    A --&gt; I(Impute data)\n    A --&gt; J(Add Date column)\n\n\n\n\nflowchart LR\n    A(Data preparation) --&gt; B(Filter for period\\n2021 to 2023)\n    A --&gt; C(Remove unused variables)\n    A --&gt; D(Check and remove duplicates)\n    A --&gt; E(Station-specific cleaning)\n    E --&gt; F(Remove stations with\\nincomplete data)\n    E --&gt; G(Remove stations with\\nno data for an entire month)\n    E --&gt; H(Remove stations with \\nexcessive missing values)\n    A --&gt; I(Impute data)\n    A --&gt; J(Add Date column)\n\n\n\n\n\n\n\n4.1 General Data Cleaning\n\n4.1.1 Check structure with glimpse()\n\nglimpse(weather)\n\nRows: 202,976\nColumns: 15\n$ Station                         &lt;chr&gt; \"Paya Lebar\", \"Paya Lebar\", \"Paya Leba…\n$ Year                            &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 20…\n$ Month                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Day                             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ `Daily Rainfall Total (mm)`     &lt;chr&gt; \"0.0\", \"0.0\", \"2.2\", \"0.6\", \"10.5\", \"3…\n$ `Highest 30 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 60 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 120 Min Rainfall (mm)` &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Mean Temperature (°C)`         &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Maximum Temperature (°C)`      &lt;chr&gt; \"29.5\", \"31.7\", \"31.1\", \"32.3\", \"27.0\"…\n$ `Minimum Temperature (°C)`      &lt;chr&gt; \"24.8\", \"25.0\", \"25.1\", \"23.7\", \"23.8\"…\n$ `Mean Wind Speed (km/h)`        &lt;chr&gt; \"15.8\", \"16.5\", \"14.9\", \"8.9\", \"11.9\",…\n$ `Max Wind Speed (km/h)`         &lt;chr&gt; \"35.3\", \"37.1\", \"33.5\", \"35.3\", \"33.5\"…\n$ LAT                             &lt;dbl&gt; 1.3524, 1.3524, 1.3524, 1.3524, 1.3524…\n$ LONG                            &lt;dbl&gt; 103.9007, 103.9007, 103.9007, 103.9007…\n\n\n\n\n4.1.2 Filter desired period\nFor this project, we focus on the year 2021 to 2023.\n\nweather &lt;- weather %&gt;%\n  filter(Year &gt;= 2021)\n\n\n\n4.1.3 Remove unused variables\nWe will drop the following columns that we will not be using for this project:\n\nHighest 30 Min Rainfall (mm)\nHighest 60 Min Rainfall (mm)\nHighest 1200 Min Rainfall (mm)\n\n\nweather &lt;- weather %&gt;%\n  select(-c(`Highest 30 Min Rainfall (mm)`, \n            `Highest 60 Min Rainfall (mm)`, \n            `Highest 120 Min Rainfall (mm)`))\n\n\n\n4.1.4 Checking for duplicated rows\nWe use the following code to check the duplicated data for combination of Station Name, Year, Month, and Day. If there is any duplicated data, it will be shown.\n\n# Identify duplicates\nduplicates &lt;- weather[duplicated(weather[c(\"Station\", \"Year\", \"Month\", \"Day\")]) | duplicated(weather[c(\"Station\", \"Year\", \"Month\", \"Day\")], fromLast = TRUE), ]\n\n# Check if 'duplicates' dataframe is empty\nif (nrow(duplicates) == 0) {\n  print(\"The combination of Station Name, Year, Month, and Day is unique.\")\n} else {\n  print(\"There are duplicates in the combination of Station Name, Year, Month, and Day. Showing duplicated rows:\")\n  # Print out the duplicated rows\n  print(duplicates)\n}\n\n[1] \"The combination of Station Name, Year, Month, and Day is unique.\"\n\n\nFor duplicated row, we remove them by using following code.\n\n# Removing duplicates and keeping the first occurrence\nweather_unique &lt;- weather %&gt;%\n  distinct(Station, Year, Month, Day, .keep_all = TRUE)\n\n# View the cleaned data\nglimpse(weather_unique)\n\nRows: 54,929\nColumns: 12\n$ Station                     &lt;chr&gt; \"Paya Lebar\", \"Paya Lebar\", \"Paya Lebar\", …\n$ Year                        &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ Month                       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Day                         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ `Daily Rainfall Total (mm)` &lt;chr&gt; \"116.0\", \"169.2\", \"3.6\", \"0.2\", \"0.0\", \"0.…\n$ `Mean Temperature (°C)`     &lt;chr&gt; \"24.5\", \"23.2\", \"24.2\", \"25.7\", \"27.5\", \"2…\n$ `Maximum Temperature (°C)`  &lt;chr&gt; \"26.8\", \"24.5\", \"25.5\", \"28.5\", \"33.2\", \"3…\n$ `Minimum Temperature (°C)`  &lt;chr&gt; \"21.7\", \"21.6\", \"23.1\", \"23.1\", \"23.4\", \"2…\n$ `Mean Wind Speed (km/h)`    &lt;chr&gt; \"12.3\", \"8.9\", \"4.9\", \"5.7\", \"11.5\", \"12.2…\n$ `Max Wind Speed (km/h)`     &lt;chr&gt; \"48.2\", \"38.9\", \"16.7\", \"18.5\", \"35.2\", \"3…\n$ LAT                         &lt;dbl&gt; 1.3524, 1.3524, 1.3524, 1.3524, 1.3524, 1.…\n$ LONG                        &lt;dbl&gt; 103.9007, 103.9007, 103.9007, 103.9007, 10…\n\n\n\n\n\n4.2 Station-Specific Data Cleaning\n\n4.2.1 Removal of Stations with Incomplete Data\nBased on the Station Records, we will remove a number of stations where there is no data for a number of variables.\n\n# Define the station names to remove\nstations_to_remove &lt;- c(\"Macritchie Reservoir\", \"Lower Peirce Reservoir\", \"Pasir Ris (West)\", \"Kampong Bahru\", \"Jurong Pier\", \"Ulu Pandan\", \"Serangoon\", \"Jurong (East)\", \"Mandai\", \"Upper Thomson\", \"Buangkok\", \"Boon Lay (West)\", \"Bukit Panjang\", \"Kranji Reservoir\", \"Tanjong Pagar\", \"Admiralty West\", \"Queenstown\", \"Tanjong Katong\", \"Chai Chee\", \"Upper Peirce Reservoir\", \"Kent Ridge\", \"Somerset (Road)\", \"Punggol\", \"Tuas West\", \"Simei\", \"Toa Payoh\", \"Tuas\", \"Bukit Timah\", \"Yishun\", \"Buona Vista\", \"Pasir Ris (Central)\", \"Jurong (North)\", \"Choa Chu Kang (West)\", \"Serangoon North\", \"Lim Chu Kang\", \"Marine Parade\", \"Choa Chu Kang (Central)\", \"Dhoby Ghaut\", \"Nicoll Highway\", \"Botanic Garden\", \"Whampoa\")\n\n# Remove rows with the specified station names\nweather_unique &lt;- weather_unique[!weather_unique$Station %in% stations_to_remove, ] # 29 Feb: replaced !weather with !weather_unique\n\nTable below shows that unique weather stations in the data table after removal of incomplete data.\n\n\nCode\n# Extract unique stations from the weather dataframe\nunique_stations &lt;- unique(weather_unique$Station)\n\nkable(unique_stations, \n      row.names = TRUE, \n      col.names = \"Station\")\n\n\n\n\n\n\nStation\n\n\n\n\n1\nPaya Lebar\n\n\n2\nSemakau Island\n\n\n3\nAdmiralty\n\n\n4\nPulau Ubin\n\n\n5\nEast Coast Parkway\n\n\n6\nMarina Barrage\n\n\n7\nAng Mo Kio\n\n\n8\nNewton\n\n\n9\nTuas South\n\n\n10\nPasir Panjang\n\n\n11\nJurong Island\n\n\n12\nChoa Chu Kang (South)\n\n\n13\nKhatib\n\n\n14\nTengah\n\n\n15\nChangi\n\n\n16\nSeletar\n\n\n17\nTai Seng\n\n\n18\nJurong (West)\n\n\n19\nClementi\n\n\n20\nSentosa Island\n\n\n21\nSembawang\n\n\n\n\n\n\n\n4.2.2 Identification and Removal of Stations with No Data for an Entire Month\nFor some stations, there is no data for an entire month. We will check and summarise this list of stations by month and year.\n\n# Count the number of distinct months with data for each Station and Year\nyearly_month_count &lt;- weather_unique %&gt;%\n  group_by(Station, Year) %&gt;%\n  summarise(TotalMonths = n_distinct(Month), .groups = 'drop')\n\n# Spread the Year to make them as columns with the count of months as values\nyearly_summary &lt;- yearly_month_count %&gt;%\n  pivot_wider(names_from = Year, values_from = TotalMonths, values_fill = list(TotalMonths = 0))\n\ndatatable(yearly_summary, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'Total number of monthly data per year for each station ')\n\n\n\n\n\nThe code below list out the missing months.\n\n# First, create a complete combination of Station, Year, and Month\nall_combinations &lt;- expand.grid(\n  Station = unique(weather_unique$Station),\n  Year = 2021:2023, # Assuming you are interested in years from 2021 onwards\n  Month = 1:12\n)\n\n# Then left join this with the original weather data to identify missing entries\nmissing_months &lt;- all_combinations %&gt;%\n  left_join(weather_unique, by = c(\"Station\", \"Year\", \"Month\")) %&gt;%\n  # Use is.na() to check for rows that didn't have a match in the original data\n  filter(is.na(Day)) %&gt;%\n  # Select only the relevant columns for the final output\n  select(Station, Year, Month)\n\n# Now, create a summary table that lists out the missing months\nmissing_months_summary &lt;- missing_months %&gt;%\n  group_by(Station, Year) %&gt;%\n  summarise(MissingMonths = toString(sort(unique(Month))), .groups = 'drop')\n\n# Print the summary\n# print(missing_months_summary)\n\ndatatable(missing_months_summary, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'List of missing months')\n\n\n\n\n\nWe will drop the data for stations that have at least one missing month.\n\n# List of station names to drop\nstations_to_drop &lt;- unique(missing_months$Station)\n\n# Filter out rows with station names in the list\nweather_unique &lt;- weather_unique %&gt;%\n  filter(!Station %in% stations_to_drop)\n\n\n\nCode\nkable(stations_to_drop, \n      row.names = TRUE, \n      col.names = \"Station\",\n      caption = \"List of Stations Dropped\")\n\n\n\nList of Stations Dropped\n\n\n\nStation\n\n\n\n\n1\nKhatib\n\n\n\n\n\n\n\nCode\nremaining_station &lt;- unique(weather_unique$Station)\n\nkable(remaining_station,\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Remaining Stations\")\n\n\n\nList of Remaining Stations\n\n\n\nStation\n\n\n\n\n1\nPaya Lebar\n\n\n2\nSemakau Island\n\n\n3\nAdmiralty\n\n\n4\nPulau Ubin\n\n\n5\nEast Coast Parkway\n\n\n6\nMarina Barrage\n\n\n7\nAng Mo Kio\n\n\n8\nNewton\n\n\n9\nTuas South\n\n\n10\nPasir Panjang\n\n\n11\nJurong Island\n\n\n12\nChoa Chu Kang (South)\n\n\n13\nTengah\n\n\n14\nChangi\n\n\n15\nSeletar\n\n\n16\nTai Seng\n\n\n17\nJurong (West)\n\n\n18\nClementi\n\n\n19\nSentosa Island\n\n\n20\nSembawang\n\n\n\n\n\n\n\n4.2.3 Checking for and Removal of Stations with Excessive Missing Values\nMissing values can be represented by “\\u0097”, “NA” and “-”.\nFirst, we filter for Station Year Month where any of the variables have 7 or more occurrences of missing values.\n\n# Assuming the previous steps to summarize and filter data\nweather_summary &lt;- weather_unique %&gt;%\n  mutate(across(where(is.numeric), ~ as.character(.))) %&gt;% # Convert numeric columns to character\n  mutate(across(-c(Station, Year, Month, Day), ~ .x == \"NA\" | .x == \"\\u0097\" | .x == \"-\", .names = \"count_{col}\")) %&gt;%\n  group_by(Station, Year = as.numeric(Year), Month = as.numeric(Month)) %&gt;%\n  summarise(across(starts_with(\"count_\"), sum), .groups = \"drop\") %&gt;%\n  # Filter rows where any count of 'NA' or '\\u0097' is 7 or more\n  filter(if_any(starts_with(\"count_\"), ~ . &gt; 7)) %&gt;%\n  # Arrange the data by Station, Year, and Month\n  arrange(Station, Year, Month)\n\n# Proceed to transform the summarized data back to a more readable format\nweather_summary_long &lt;- weather_summary %&gt;%\n  pivot_longer(-c(Station, Year, Month), names_to = \"Measurement\", values_to = \"Count_NA_U0097\") %&gt;%\n  separate(Measurement, into = c(\"Dummy\", \"Variable\"), sep = \"_\", extra = \"merge\") %&gt;%\n  select(-Dummy) %&gt;%\n  pivot_wider(names_from = \"Variable\", values_from = \"Count_NA_U0097\")\n\n# View the ordered, filtered summary\n# print(weather_summary_long)\n\ndatatable(weather_summary_long, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'List of missing values per month')\n\n\n\n\n\nWe summarize the total number of months with missing data per station.\n\nstation_missing_summary &lt;- weather_summary %&gt;%\n  group_by(Station) %&gt;%\n  summarise(Total_Missing_Months = n()) %&gt;%\n  ungroup()\n\ndatatable(station_missing_summary, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'Total missing months with more than 7 missing values')\n\n\n\n\n\nAs the station has more than 7 missing values each month, we drop the station with the following code.\n\n# Drop stations in weather_unique that are listed in station_missing_summary\nweather_unique_filtered &lt;- anti_join(weather_unique, station_missing_summary, by = \"Station\")\n\n\n\nCode\nStation_excessive_missing &lt;- unique(station_missing_summary$Station)\n                                    \nkable(Station_excessive_missing, \n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Stations Dropped\")\n\n\n\nList of Stations Dropped\n\n\n\nStation\n\n\n\n\n1\nAdmiralty\n\n\n2\nClementi\n\n\n3\nJurong Island\n\n\n4\nMarina Barrage\n\n\n5\nNewton\n\n\n6\nPasir Panjang\n\n\n7\nPaya Lebar\n\n\n8\nSemakau Island\n\n\n9\nSembawang\n\n\n10\nSentosa Island\n\n\n11\nTai Seng\n\n\n12\nTengah\n\n\n13\nTuas South\n\n\n\n\n\n\n\n\n4.3 Final data assessment\nAfter previous steps, the list below shows the final stations that we will be using in the subsequent analysis. This section, we will first examine the final data set after removal of stations. Next, summary of missing values will be shown before we proceed with data imputation.\n\n\nCode\nfinal_stations &lt;- unique(weather_unique_filtered$Station)\n\nkable(final_stations,\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Final Stations\")\n\n\n\nList of Final Stations\n\n\n\nStation\n\n\n\n\n1\nPulau Ubin\n\n\n2\nEast Coast Parkway\n\n\n3\nAng Mo Kio\n\n\n4\nChoa Chu Kang (South)\n\n\n5\nChangi\n\n\n6\nSeletar\n\n\n7\nJurong (West)\n\n\n\n\n\nFollowing data table show the final data set.\n\ndatatable(weather_unique_filtered, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'Final data set')\n\n\n\n\n\nThe table below shows the numbers of missing value of the final data set.\n\n# Assuming the previous steps to summarize and filter data\nweather_summary &lt;- weather_unique_filtered %&gt;%\n  mutate(across(where(is.numeric), ~ as.character(.))) %&gt;% # Convert numeric columns to character\n  mutate(across(-c(Station, Year, Month, Day), ~ .x == \"NA\" | .x == \"\\u0097\" | .x == \"-\", .names = \"count_{col}\")) %&gt;%\n  group_by(Station, Year = as.numeric(Year), Month = as.numeric(Month)) %&gt;%\n  summarise(across(starts_with(\"count_\"), sum), .groups = \"drop\") %&gt;%\n  # Filter rows where any count of 'NA' or '\\u0097' is 7 or more\n  filter(if_any(starts_with(\"count_\"), ~ . &gt; 0 & . &lt;= 7)) %&gt;%\n  # Arrange the data by Station, Year, and Month\n  arrange(Station, Year, Month)\n\n# Proceed to transform the summarized data back to a more readable format\nweather_summary_long &lt;- weather_summary %&gt;%\n  pivot_longer(-c(Station, Year, Month), names_to = \"Measurement\", values_to = \"Count_NA_U0097\") %&gt;%\n  separate(Measurement, into = c(\"Dummy\", \"Variable\"), sep = \"_\", extra = \"merge\") %&gt;%\n  select(-Dummy) %&gt;%\n  pivot_wider(names_from = \"Variable\", values_from = \"Count_NA_U0097\")\n\n# View the ordered, filtered summary\n# print(weather_summary_long)\n\ndatatable(weather_summary_long, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'List of missing values per month for final data set')\n\n\n\n\n\n\n\n4.4 Imputation of data\n\n# Ensure Date column exists\nweather_unique_filtered$Date &lt;- as.Date(paste(weather_unique_filtered$Year, weather_unique_filtered$Month, weather_unique_filtered$Day, sep = \"-\"))\n\n# Define the weather variables to loop through\nweather_variables &lt;- c(\"Daily Rainfall Total (mm)\", \"Mean Temperature (°C)\", \"Maximum Temperature (°C)\", \n                       \"Minimum Temperature (°C)\", \"Mean Wind Speed (km/h)\", \"Max Wind Speed (km/h)\")\n\n# Loop through each weather variable to impute missing values\nfor(variable in weather_variables) {\n  # Convert variable to numeric, as na_ma() requires numeric input\n  weather_unique_filtered[[variable]] &lt;- as.numeric(as.character(weather_unique_filtered[[variable]]))\n  \n  # Impute missing values using a moving average\n  weather_unique_imputed &lt;- weather_unique_filtered %&gt;%\n    group_by(Station) %&gt;%\n    arrange(Station, Date) %&gt;%\n    mutate(!!variable := round(na_ma(!!sym(variable), k = 3, weighting = \"simple\"), 1)) %&gt;%\n    ungroup()\n}\n\n# Drop 'Year', 'Month', 'Day' columns and rearrange so 'Date' is the second column, immediately after 'Station'\nweather_unique_imputed &lt;- weather_unique_imputed %&gt;%\n  select(Station, Date, everything(), -Year, -Month, -Day)\n\n\n\n4.5 Add Date column to data\n\n# Ensure Date column exists\nweather_unique_filtered$Date &lt;- as.Date(paste(weather_unique_filtered$Year, weather_unique_filtered$Month, weather_unique_filtered$Day, sep = \"-\"))\n\nweather_variables &lt;- c(\"Daily Rainfall Total (mm)\", \"Mean Temperature (°C)\", \"Maximum Temperature (°C)\", \n                       \"Minimum Temperature (°C)\", \"Mean Wind Speed (km/h)\", \"Max Wind Speed (km/h)\")\n\nweather_unique_imputed &lt;- weather_unique_filtered\n\nfor(variable in weather_variables) {\n  weather_unique_imputed[[variable]] &lt;- as.numeric(as.character(weather_unique_imputed[[variable]]))\n  \n  # Impute missing values using a moving average, rounded to 1 decimal place\n  weather_unique_imputed &lt;- weather_unique_imputed %&gt;%\n    group_by(Station) %&gt;%\n    arrange(Station, Date) %&gt;%\n    mutate(!!variable := round(na_ma(!!sym(variable), k = 3, weighting = \"simple\"), 1)) %&gt;%\n    ungroup()\n}\n\n# Drop 'Year', 'Month', 'Day' columns and rearrange so 'Date' is the second column\nweather_unique_imputed &lt;- weather_unique_imputed %&gt;%\n  select(Station, Date, everything(), -Year, -Month, -Day)\n\n\ndatatable(weather_unique_imputed, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10,scrollX=T),\n          caption = 'Final data set after imputation')"
  },
  {
    "objectID": "code/Data_preparation_Weather.html#save-data-to-rds",
    "href": "code/Data_preparation_Weather.html#save-data-to-rds",
    "title": "Data Preparation",
    "section": "5 Save data to rds",
    "text": "5 Save data to rds\nFollowing code is used to export the imputed data table to csv.\n\nwrite_rds(weather_unique_imputed, \"../data/weather_imputed.rds\")\n\nTo read data in the future, use the following code chunk:\n\nweather_data &lt;- read_rds(\"../data/weather_imputed.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AirWeather Analytics",
    "section": "",
    "text": "In Singapore, while there has been significant research on climate change, the relationship between changing weather patterns and air quality has not been thoroughly explored. While The National Environment Agency in Singapore has made historical weather and air quality data accessible, there remains a shortage of user-friendly tools to effectively visualize and analyze this data for a deeper understanding of its implications. Exploring these datasets within a visual analytics environment can provide valuable insights for various users, from policymakers to the general public.\nAt present, there are few or limited visualization tools available to explore Singapore’s historical weather and air quality data. The few tools available are constrained by static displays and limited analytical depth, and do not capture the complex temporal and spatial nature of the data. They also lack of interactivity or advanced analytical functions, and are unable to explore the relationship between air pollutants and weather parameters.\nTo address this gap, we will attempt to develop an interactive R Shiny application tailored for the exploration of Singapore’s historical weather and air quality data.\nRead our proposal here."
  }
]